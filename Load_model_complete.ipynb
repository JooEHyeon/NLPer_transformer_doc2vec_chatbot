{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Load_model_complete.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "15mqS4GiG2Fkp1MMBFcnTpGBWs3ZLwfF7",
      "authorship_tag": "ABX9TyNK+GZojDgr4Q9Te7F3nIFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ideablast/NLPer_transformer_doc2vec_chatbot/blob/kdg/Load_model_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pAKMKWP7gkr",
        "outputId": "4ea6321d-d6cb-471f-ce84-63933a332fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 47.3MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: tweepy, JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4Iq6N6s7vKX"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers, losses, metrics\n",
        "from keras import preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "# from hanspell import spell_checker\n",
        "# from pykospacing import spacing\n",
        "\n",
        "import pickle"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhKvbowK7lGm"
      },
      "source": [
        "# 태그 단어\n",
        "PAD = \"<PADDING>\"   # 패딩\n",
        "STA = \"<START>\"     # 시작\n",
        "END = \"<END>\"       # 끝\n",
        "OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
        "\n",
        "# 태그 인덱스\n",
        "PAD_INDEX = 0\n",
        "STA_INDEX = 1\n",
        "END_INDEX = 2\n",
        "OOV_INDEX = 3\n",
        "\n",
        "# 데이터 타입\n",
        "ENCODER_INPUT  = 0\n",
        "DECODER_INPUT  = 1\n",
        "DECODER_TARGET = 2\n",
        "\n",
        "# Hyper-parameters for Transformer\n",
        "NUM_LAYERS = 2 # Encdoer, Decoder layer수(각각)\n",
        "D_MODEL = 256 # word embedding dimension\n",
        "NUM_HEADS = 8 # attention 헤드 수. D_Model % NUM_HEADS == 0이 되야 함!\n",
        "UNITS = 512 # FFNN 유닛수\n",
        "DROPOUT = 0.1 #dropout rate\n",
        "EPOCHS = 50 ## Transformer, C,M Classification 에폭(에너르기폭발)\n",
        "# for Transformer data pipelining\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "# VOCAB_SIZE = 0 # 단어사전이 보유한 단어의 개수. 후에 len(words) 로 바뀜.\n",
        "# 한 문장에서 단어의 최대 개수\n",
        "max_sequences = 30\n",
        "# 정규 표현식 필터\n",
        "RE_FILTER = re.compile(\"[\\\"':;~()]\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhVVnW_uEWT1"
      },
      "source": [
        "## functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbivSckMydzN"
      },
      "source": [
        "# 형태소분석 함수\n",
        "def pos_tag(sentences):\n",
        "    \n",
        "    # KoNLPy 형태소분석기 설정\n",
        "    tagger = Okt()\n",
        "    \n",
        "    # 문장 품사 변수 초기화\n",
        "    sentences_pos = []\n",
        "    \n",
        "    # 모든 문장 반복\n",
        "    for sentence in sentences:\n",
        "        # [\\\"':;~()] 특수기호 제거\n",
        "        sentence = re.sub(RE_FILTER, \"\", sentence)\n",
        "        \n",
        "        # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
        "        sentence = \" \".join(tagger.morphs(sentence))\n",
        "        sentences_pos.append(sentence)\n",
        "        \n",
        "    return sentences_pos"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfoTztrvydzc"
      },
      "source": [
        "# 문장을 인덱스로 변환\n",
        "def convert_text_to_index(sentences, vocabulary, type): \n",
        "    \n",
        "    sentences_index = []\n",
        "    \n",
        "    # 모든 문장에 대해서 반복\n",
        "    for sentence in sentences:\n",
        "        sentence_index = []\n",
        "        \n",
        "        # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
        "        if type == DECODER_INPUT:\n",
        "            sentence_index.extend([vocabulary[STA]])\n",
        "        \n",
        "        # 문장의 단어들을 띄어쓰기로 분리\n",
        "        for word in sentence.split():\n",
        "            if vocabulary.get(word) is not None:\n",
        "                # 사전에 있는 단어면 해당 인덱스를 추가\n",
        "                sentence_index.extend([vocabulary[word]])\n",
        "            else:\n",
        "                # 사전에 없는 단어면 OOV 인덱스를 추가\n",
        "                sentence_index.extend([vocabulary[OOV]])\n",
        "\n",
        "        # 최대 길이 검사\n",
        "        if type == DECODER_TARGET:\n",
        "            # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
        "            if len(sentence_index) >= max_sequences:\n",
        "                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]\n",
        "            else:\n",
        "                sentence_index += [vocabulary[END]]\n",
        "        else:\n",
        "            if len(sentence_index) > max_sequences:\n",
        "                sentence_index = sentence_index[:max_sequences]\n",
        "            \n",
        "        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
        "        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]\n",
        "        \n",
        "        # 문장의 인덱스 배열을 추가\n",
        "        sentences_index.append(sentence_index)\n",
        "\n",
        "    return np.asarray(sentences_index)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRWIt1c6PMHf"
      },
      "source": [
        "## custom function for Transformer model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-KBi2sLPPwz"
      },
      "source": [
        "## scaled dot product Attention\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True) # QK^T\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth) #  QK^T / sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9) # zero padding token softmax 결과가 0이 나오도록\n",
        "  \n",
        "  attention_weights = tf.nn.softmax(logits, axis = -1) # softmax(QK^T / sqrt(d_k))\n",
        "\n",
        "  output = tf.matmul(attention_weights, value) # softmax(QK^T / sqrt(d_k)) * V\n",
        "\n",
        "  return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIvQhbVcPV1l"
      },
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KeKurrYPW0K"
      },
      "source": [
        "# it handle mask future tokens in a sequence used decoder. and mask pad tokens\n",
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGjibIdt8E4o"
      },
      "source": [
        "## Load models(Transformer, 2 bilstm) & dictionraies(6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl02IOPZcrTN",
        "outputId": "2755b214-1330-42dd-fd6a-9a44b26c69d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv2hxlqhO8bp"
      },
      "source": [
        "t_model = models.load_model('/content/drive/My Drive/Transformer_text_savedmodelform', compile=False)\n",
        "m_model = models.load_model('/content/drive/My Drive/main_lstm_cl_test.h5')\n",
        "c_model = models.load_model('/content/drive/My Drive/category_lstm_cl_test.h5')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e-P9mz-S7Wk"
      },
      "source": [
        "with open('/content/drive/My Drive/dictionary_list.pickle', 'rb') as handle:\n",
        "  dictionary_list = pickle.load(handle)\n",
        "\n",
        "word_to_index = dictionary_list[0]\n",
        "index_to_word = dictionary_list[1]\n",
        "category_to_index = dictionary_list[2]\n",
        "index_to_category = dictionary_list[3]\n",
        "main_to_index = dictionary_list[4]\n",
        "index_to_main = dictionary_list[5]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NthODiGlcHnz"
      },
      "source": [
        "#input : '이 옷 다른 사이즈도 볼 수 있을까요?'\n",
        "#output : ('의류', 1.0)\n",
        "def show_prob_c(stc):\n",
        "  list_stc = [stc]\n",
        "  pos_stc = pos_tag(list_stc)\n",
        "  index_stc = convert_text_to_index(pos_stc, word_to_index, 0).reshape(1,30)\n",
        "  logits = c_model.predict(index_stc)\n",
        "\n",
        "  index = np.argmax(logits)\n",
        "  probability = np.max(logits)\n",
        "\n",
        "  return index_to_category[index], probability"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKPWrXYw8TsN"
      },
      "source": [
        "#input : '이 옷 다른 사이즈도 볼 수 있을까요?'\n",
        "#output : ('치수문의', 0.7858745)\n",
        "def show_prob_m(stc):\n",
        "  list_stc = [stc]\n",
        "  pos_stc = pos_tag(list_stc)\n",
        "  index_stc = convert_text_to_index(pos_stc, word_to_index, 0).reshape(1,30)\n",
        "  logits = m_model.predict(index_stc)\n",
        "\n",
        "  index = np.argmax(logits)\n",
        "  probability = np.max(logits)\n",
        "\n",
        "  return index_to_main[index], probability"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLU17mJSVffj"
      },
      "source": [
        "# input : '남성 바지는 어느 쪽에 있나요?'\n",
        "# output : '저 뒤쪽 에 있어요'\n",
        "def Transformer_prediction(stc):\n",
        "  list_stc = [stc]\n",
        "  pos_stc = pos_tag(list_stc)\n",
        "  index_stc = convert_text_to_index(pos_stc, word_to_index, ENCODER_INPUT)\n",
        "  input_seq = index_stc.squeeze()\n",
        "  sentence = tf.expand_dims(input_seq, axis=0) # make tensor type\n",
        "  output = tf.expand_dims([1], 0)\n",
        "\n",
        "  for i in range(max_sequences):\n",
        "    predictions = t_model.predict([sentence, output])\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    if tf.equal(predicted_id, 2):\n",
        "      break\n",
        "\n",
        "    # concatenated the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  output_indexes = tf.squeeze(output, axis=0)[1:].numpy()\n",
        "  sentence = ''\n",
        "  # 모든 문장에 대해서 반복\n",
        "  for index in output_indexes:\n",
        "      if index == END_INDEX:\n",
        "          # 종료 인덱스면 중지\n",
        "          break;\n",
        "      if index_to_word.get(index) is not None:\n",
        "          # 사전에 있는 인덱스면 해당 단어를 추가\n",
        "          sentence += index_to_word[index]\n",
        "      else:\n",
        "          # 사전에 없는 인덱스면 OOV 단어를 추가\n",
        "          sentence.extend([index_to_word[OOV_INDEX]])\n",
        "\n",
        "      sentence += ' '\n",
        "          \n",
        "  return sentence"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TElakKd5b6Jj"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kntm06Vl8U7O"
      },
      "source": [
        "stc = '남성 바지는 어느 쪽에 있나요?'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bDoIOX08WG-",
        "outputId": "93b8fde8-6913-4895-a59e-b2f438cd770d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "show_prob_c(stc), show_prob_m(stc)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('의류', 1.0), ('종류별의류제품문의요청', 0.98762506))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnurNuc1Yhoc",
        "outputId": "cd717b59-0b53-4af3-c4a4-578b757f04d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ans = Transformer_prediction(stc)\n",
        "print(ans) \n",
        "show_prob_c(ans),show_prob_m(ans) "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd6e3c0b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "저 뒤쪽 에 있어요 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('의류', 0.9849696), ('종류별액세서리제품문의요청', 0.9806169))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjOzBf7S8YCh",
        "outputId": "8b1b48b9-f31b-4566-960c-c9ddaf46a0e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stc = input()\n",
        "print(\"입력하신 문장의 카테고리,의도 : \",show_prob_c(stc), show_prob_m(stc))\n",
        "ans = Transformer_prediction(stc)\n",
        "print(\"AI 답변 :\", ans)\n",
        "print(\"AI 답변의 카테고리,의도 : \", show_prob_c(ans), show_prob_m(ans) )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "이거 얼마에요?\n",
            "입력하신 문장의 카테고리,의도 :  ('가방', 0.41945946) ('제품가격문의', 0.99989533)\n",
            "AI 답변 : 네 20000원 입니다 \n",
            "AI 답변의 카테고리,의도 :  ('의류', 0.9999721) ('제품가격문의', 0.98077536)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5VsSRr4bypX"
      },
      "source": [
        ""
      ],
      "execution_count": 123,
      "outputs": []
    }
  ]
}