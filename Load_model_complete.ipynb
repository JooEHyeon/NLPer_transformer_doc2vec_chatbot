{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Load_model_complete.ipynb",
      "provenance": [],
      "mount_file_id": "15mqS4GiG2Fkp1MMBFcnTpGBWs3ZLwfF7",
      "authorship_tag": "ABX9TyNwvaJ7TN/H7UvOl7kRhs40",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ideablast/NLPer_transformer_doc2vec_chatbot/blob/kdg/Load_model_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pAKMKWP7gkr",
        "outputId": "d8ee7852-4ae4-4deb-e7f6-9be165c4834d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-sc0vrxux\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-sc0vrxux\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.3) (2.3.0)\n",
            "Requirement already satisfied: keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.3) (2.4.3)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.3) (2.10.0)\n",
            "Collecting argparse>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (1.15.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (0.35.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (1.12.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (1.18.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (1.33.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.3) (2.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.4.3->pykospacing==0.3) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=2.3.0->pykospacing==0.3) (50.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (0.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (2.0.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.3) (3.1.0)\n",
            "Building wheels for collected packages: pykospacing\n",
            "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykospacing: filename=pykospacing-0.3-cp36-none-any.whl size=2255638 sha256=6b5e1bea054e5eca8c234ad72ad78137663e3aa02217fe1fdf374c1e723d55d3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oxm9glwu/wheels/4d/45/58/e26cb2b7f6a063d234158c6fd1e5700f6e15b99d67154340ba\n",
            "Successfully built pykospacing\n",
            "Installing collected packages: argparse, pykospacing\n",
            "Successfully installed argparse-1.4.0 pykospacing-0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-5ibbn124\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-5ibbn124\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp36-none-any.whl size=4854 sha256=a33961ddf032c76a591d52f04843a09e449de2be729258194c5a279c1b064a2c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a6k3_qj2/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n",
            "Successfully built py-hanspell\n",
            "Installing collected packages: py-hanspell\n",
            "Successfully installed py-hanspell-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4Iq6N6s7vKX"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers, losses, metrics\n",
        "from keras import preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from gensim.models import Doc2Vec\n",
        "from konlpy.tag import Okt, Kkma\n",
        "from hanspell import spell_checker\n",
        "from pykospacing import spacing\n",
        "import jpype\n",
        "\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore') "
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhKvbowK7lGm"
      },
      "source": [
        "# 태그 단어\n",
        "PAD = \"<PADDING>\"   # 패딩\n",
        "STA = \"<START>\"     # 시작\n",
        "END = \"<END>\"       # 끝\n",
        "OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
        "\n",
        "# 태그 인덱스\n",
        "PAD_INDEX = 0\n",
        "STA_INDEX = 1\n",
        "END_INDEX = 2\n",
        "OOV_INDEX = 3\n",
        "\n",
        "# 데이터 타입\n",
        "ENCODER_INPUT  = 0\n",
        "DECODER_INPUT  = 1\n",
        "DECODER_TARGET = 2\n",
        "\n",
        "# Hyper-parameters for Transformer\n",
        "NUM_LAYERS = 2                       # Encdoer, Decoder layer수(각각)\n",
        "D_MODEL = 256                        # word embedding dimension\n",
        "NUM_HEADS = 8                        # attention 헤드 수. D_Model % NUM_HEADS == 0이 되야 함!\n",
        "UNITS = 512                          # FFNN 유닛수\n",
        "DROPOUT = 0.1                        # dropout rate\n",
        "EPOCHS = 50                          # Transformer, C,M Classification 에폭(에너르기폭발)\n",
        "BATCH_SIZE = 64                      # Batch_size\n",
        "BUFFER_SIZE = 1000                   # for data pipelining\n",
        "# VOCAB_SIZE = 0                     # 단어사전이 보유한 단어의 개수. 후에 len(words) 로 바뀜.\n",
        "max_sequences = 30                   # 한 문장에서 단어의 최대 개수\n",
        "RE_FILTER = re.compile(\"[\\\"':;~()]\") # 정규 표현식 필터"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhVVnW_uEWT1"
      },
      "source": [
        "## functions\n",
        " - pos_tag([sentence]) : 형태소 분석+불용어제거된 문장 반환\n",
        " - convert_text_to_index([sentence], word_to_index, TYPE) : 정수인코딩+패딩된 문장을 반환, Type = 0:일반문장 1: 트랜스포머에 들어갈 문장, 2 : 트랜스포머에 들어갈 디코더 문장(학습할때만 이용)\n",
        " - grammar_checker(sentence) : 띄어쓰기, 문법교정된 문장 반환\n",
        " - show_prob_c(sentence) : 문장의 카테고리, 확률 반환\n",
        " - show_prob_m(sentence) : 문장의 의도, 확률 반환\n",
        " - Transformer_prediction(sentence) : 문장의 답변 반환\n",
        " - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbivSckMydzN"
      },
      "source": [
        "# 형태소분석 함수\n",
        "def pos_tag(sentences):\n",
        "    \n",
        "    # KoNLPy 형태소분석기 설정\n",
        "    tagger = Okt()\n",
        "    \n",
        "    # 문장 품사 변수 초기화\n",
        "    sentences_pos = []\n",
        "    \n",
        "    # 모든 문장 반복\n",
        "    for sentence in sentences:\n",
        "        # [\\\"':;~()] 특수기호 제거\n",
        "        sentence = re.sub(RE_FILTER, \"\", sentence)\n",
        "        \n",
        "        # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
        "        sentence = \" \".join(tagger.morphs(sentence))\n",
        "        sentences_pos.append(sentence)\n",
        "        \n",
        "    return sentences_pos"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfoTztrvydzc"
      },
      "source": [
        "# 문장을 인덱스로 변환\n",
        "def convert_text_to_index(sentences, vocabulary, type): \n",
        "    \n",
        "    sentences_index = []\n",
        "    \n",
        "    # 모든 문장에 대해서 반복\n",
        "    for sentence in sentences:\n",
        "        sentence_index = []\n",
        "        \n",
        "        # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
        "        if type == DECODER_INPUT:\n",
        "            sentence_index.extend([vocabulary[STA]])\n",
        "        \n",
        "        # 문장의 단어들을 띄어쓰기로 분리\n",
        "        for word in sentence.split():\n",
        "            if vocabulary.get(word) is not None:\n",
        "                # 사전에 있는 단어면 해당 인덱스를 추가\n",
        "                sentence_index.extend([vocabulary[word]])\n",
        "            else:\n",
        "                # 사전에 없는 단어면 OOV 인덱스를 추가\n",
        "                sentence_index.extend([vocabulary[OOV]])\n",
        "\n",
        "        # 최대 길이 검사\n",
        "        if type == DECODER_TARGET:\n",
        "            # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
        "            if len(sentence_index) >= max_sequences:\n",
        "                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]\n",
        "            else:\n",
        "                sentence_index += [vocabulary[END]]\n",
        "        else:\n",
        "            if len(sentence_index) > max_sequences:\n",
        "                sentence_index = sentence_index[:max_sequences]\n",
        "            \n",
        "        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
        "        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]\n",
        "        \n",
        "        # 문장의 인덱스 배열을 추가\n",
        "        sentences_index.append(sentence_index)\n",
        "\n",
        "    return np.asarray(sentences_index)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-idAhqaew9HH"
      },
      "source": [
        "def grammar_checker(sentence):\n",
        "\n",
        "  spacing_sentence = spacing(sentence.replace(' ',''))\n",
        "  spelled_sentence = spell_checker.check(spacing_sentence)\n",
        "  checked_sentence = spelled_sentence.checked\n",
        "\n",
        "  return checked_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzr0zmnjdQYP"
      },
      "source": [
        "#input : '이 옷 다른 사이즈도 볼 수 있을까요?'\n",
        "#output : ('의류', 1.0)\n",
        "def show_prob_c(stc):\n",
        "  list_stc = [stc]\n",
        "  pos_stc = pos_tag(list_stc)\n",
        "  index_stc = convert_text_to_index(pos_stc, word_to_index, 0).reshape(1,30)\n",
        "  logits = c_model.predict(index_stc)\n",
        "\n",
        "  index = np.argmax(logits)\n",
        "  probability = np.max(logits)\n",
        "\n",
        "  return index_to_category[index], probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1mk5C6_dxiu"
      },
      "source": [
        "#input : '이 옷 다른 사이즈도 볼 수 있을까요?'\n",
        "#output : ('치수문의', 0.7858745)\n",
        "def show_prob_m(stc):\n",
        "  list_stc = [stc]\n",
        "  pos_stc = pos_tag(list_stc)\n",
        "  index_stc = convert_text_to_index(pos_stc, word_to_index, 0).reshape(1,30)\n",
        "  logits = m_model.predict(index_stc)\n",
        "\n",
        "  index = np.argmax(logits)\n",
        "  probability = np.max(logits)\n",
        "\n",
        "  return index_to_main[index], probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j22o8pyd9iH"
      },
      "source": [
        "# input : '남성 바지는 어느 쪽에 있나요?'\n",
        "# output : '저 뒤쪽 에 있어요'\n",
        "def Transformer_prediction(stc):\n",
        "  list_stc = [stc]\n",
        "  pos_stc = pos_tag(list_stc)\n",
        "  index_stc = convert_text_to_index(pos_stc, word_to_index, ENCODER_INPUT)\n",
        "  input_seq = index_stc.squeeze()\n",
        "  sentence = tf.expand_dims(input_seq, axis=0) # make tensor type\n",
        "  output = tf.expand_dims([1], 0)\n",
        "\n",
        "  for i in range(max_sequences):\n",
        "    predictions = t_model.predict([sentence, output])\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    if tf.equal(predicted_id, 2):\n",
        "      break\n",
        "\n",
        "    # concatenated the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  output_indexes = tf.squeeze(output, axis=0)[1:].numpy()\n",
        "  sentence = ''\n",
        "  # 모든 문장에 대해서 반복\n",
        "  for index in output_indexes:\n",
        "      if index == END_INDEX:\n",
        "          # 종료 인덱스면 중지\n",
        "          break;\n",
        "      if index_to_word.get(index) is not None:\n",
        "          # 사전에 있는 인덱스면 해당 단어를 추가\n",
        "          sentence += index_to_word[index]\n",
        "      else:\n",
        "          # 사전에 없는 인덱스면 OOV 단어를 추가\n",
        "          sentence.extend([index_to_word[OOV_INDEX]])\n",
        "\n",
        "      sentence += ' '\n",
        "          \n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VYACvyYee0H"
      },
      "source": [
        "kkma = Kkma()\n",
        "def tokenizer_kkma(doc):\n",
        "    # 꼬꼬마 형태소 분석기가 자바 기반이어서 파이썬에서 자바함수들을 실행할 수 있는 명령어 (jpype) 를 써줘야한다.\n",
        "    jpype.attachThreadToJVM()       \n",
        "    token_doc = [\"/\".join(word) for word in kkma.pos(doc)]\n",
        "    return token_doc"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIqOWiQcegqe"
      },
      "source": [
        "def doc2_answer(input_question):\n",
        "  token_test = tokenizer_kkma(input_question)\n",
        "  predict_vector = d2v_faqs.infer_vector(token_test)\n",
        "  result = d2v_faqs.docvecs.most_similar([predict_vector],topn=1)\n",
        "  return faqs[int(result[0][0])-1][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EImtoqf8v3vB"
      },
      "source": [
        "def score_calcul(left_cate,right_cate, num):#카테고리를 두개를 입력하면 유사도를 계산함, num : 의도면 1, 카테고리면 0\n",
        "  result = 0\n",
        "  if left_cate[0]==right_cate[0]:\n",
        "      if round(abs(left_cate[1]-right_cate[1]),3) == 0:\n",
        "          result+=1000\n",
        "      else:\n",
        "          result += (1/round(abs(left_cate[1]-right_cate[1]),3)) # 최대 999 이상 나올 수 없다.\n",
        "\n",
        "  if num == 1 and result > 0:\n",
        "      result+=500\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJda0oFsvyML"
      },
      "source": [
        "# 문장을 입력으로 넣으면\n",
        "# 최종 마지막 답변만 출력 되도록 변경\n",
        "def result_final(stc):\n",
        "    result = \"\"\n",
        "    doc2_score=0\n",
        "    tran_score=0\n",
        "\n",
        "    q_category, q_cprob = show_prob_c(stc)\n",
        "    q_main, q_mprob = show_prob_m(stc)\n",
        "\n",
        "    T_answer = Transformer_prediction(stc)\n",
        "    D_answer = doc2_answer(stc)  \n",
        "\n",
        "    T_category, T_cprob = show_prob_c(T_answer)\n",
        "    T_main, T_mprob = show_prob_m(T_answer)\n",
        "\n",
        "    D_category, D_cprob = show_prob_c(D_answer)\n",
        "    D_main, D_mprob = show_prob_m(D_answer)\n",
        "\n",
        "    tran_score = score_calcul([q_category, q_cprob], [T_category, T_cprob], 0) + score_calcul([q_main, q_mprob], [T_main, T_mprob], 1)\n",
        "    doc2_score = score_calcul([q_category, q_cprob], [D_category, D_cprob], 0) + score_calcul([q_main, q_mprob], [D_main, D_mprob], 1)\n",
        "\n",
        "    if doc2_score > tran_score:\n",
        "        result = grammar_checker(D_answer)\n",
        "    elif tran_score > doc2_score:\n",
        "        result = grammar_checker(T_answer)\n",
        "    else:\n",
        "        result = \"잘모르겠습니다\"\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRWIt1c6PMHf"
      },
      "source": [
        "## custom function for Transformer model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-KBi2sLPPwz"
      },
      "source": [
        "## scaled dot product Attention\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True) # QK^T\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth) #  QK^T / sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9) # zero padding token softmax 결과가 0이 나오도록\n",
        "  \n",
        "  attention_weights = tf.nn.softmax(logits, axis = -1) # softmax(QK^T / sqrt(d_k))\n",
        "\n",
        "  output = tf.matmul(attention_weights, value) # softmax(QK^T / sqrt(d_k)) * V\n",
        "\n",
        "  return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIvQhbVcPV1l"
      },
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KeKurrYPW0K"
      },
      "source": [
        "# it handle mask future tokens in a sequence used decoder. and mask pad tokens\n",
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGjibIdt8E4o"
      },
      "source": [
        "## Load models(Transformer, 2 bilstm) & dictionraies(6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl02IOPZcrTN",
        "outputId": "dab0160b-7f2e-40af-fd0e-0516b143e8b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv2hxlqhO8bp"
      },
      "source": [
        "t_model = models.load_model('/content/drive/My Drive/Transformer_text_savedmodelform', compile=False)\n",
        "m_model = models.load_model('/content/drive/My Drive/main_lstm_cl_test.h5')\n",
        "c_model = models.load_model('/content/drive/My Drive/category_lstm_cl_test.h5')\n",
        "d2v_faqs = Doc2Vec.load('/content/drive/My Drive/My_Doc2vec.model')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e-P9mz-S7Wk"
      },
      "source": [
        "with open('/content/drive/My Drive/dictionary_list.pickle', 'rb') as handle:\n",
        "  dictionary_list = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/data.pickle', 'rb') as f:\n",
        "  faqs = pickle.load(f)\n",
        "\n",
        "word_to_index = dictionary_list[0]\n",
        "index_to_word = dictionary_list[1]\n",
        "category_to_index = dictionary_list[2]\n",
        "index_to_category = dictionary_list[3]\n",
        "main_to_index = dictionary_list[4]\n",
        "index_to_main = dictionary_list[5]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TElakKd5b6Jj"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUI9JBbwud6I",
        "outputId": "2a4991c7-6a90-4d07-8604-39215cf55b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "result_final(\"이 신발 얼마에요?\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이게 6만 9천 원이에요'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    }
  ]
}