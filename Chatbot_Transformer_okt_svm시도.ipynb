{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Chatbot_Transformer_okt_svm시도.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9UA-8fQ2EDnp",
        "ioT3c4eLEKon",
        "HhVVnW_uEWT1",
        "lsf4tJl61Xi9",
        "Pav-8Jd3ydzp"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ideablast/NLPer_chatbot/blob/toram/Chatbot_Transformer_okt_svm%EC%8B%9C%EB%8F%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsiilWsMNHHL",
        "outputId": "aa419828-90cb-42cb-be86-420d1840863c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.4MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.6MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 44.4MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, JPype1, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UA-8fQ2EDnp"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DAFy5c7ydzC"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers, losses, metrics\n",
        "from keras import preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioT3c4eLEKon"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9YUdE1OydzG"
      },
      "source": [
        "# 태그 단어\n",
        "PAD = \"<PADDING>\"   # 패딩\n",
        "STA = \"<START>\"     # 시작\n",
        "END = \"<END>\"       # 끝\n",
        "OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
        "\n",
        "# 태그 인덱스\n",
        "PAD_INDEX = 0\n",
        "STA_INDEX = 1\n",
        "END_INDEX = 2\n",
        "OOV_INDEX = 3\n",
        "\n",
        "# 데이터 타입\n",
        "ENCODER_INPUT  = 0\n",
        "DECODER_INPUT  = 1\n",
        "DECODER_TARGET = 2\n",
        "\n",
        "# Hyper-parameters\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256 ##word embedding dim\n",
        "NUM_HEADS = 8 ## D_Model % NUM_HEADS == 0이 되야하므로...\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "EPOCHS = 50\n",
        "# for data pipelining\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "VOCAB_SIZE = 0 # 후에 len(words) 로 바뀜.\n",
        "\n",
        "# 한 문장에서 단어 시퀀스의 최대 개수\n",
        "max_sequences = 30\n",
        "\n",
        "# 정규 표현식 필터\n",
        "RE_FILTER = re.compile(\"[\\\"':;~()]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhVVnW_uEWT1"
      },
      "source": [
        "## Data Load & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80IauV0FpUv2",
        "outputId": "dd527d78-cf1e-4116-b1a5-d9171c83aaec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEHDnPXNzZlM",
        "outputId": "e26c2dc4-e27e-4116-cea2-49d89fd4300a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "wear_data = pd.read_csv(\"/content/drive/My Drive/wear.csv\")\n",
        "print(wear_data.shape)\n",
        "customer = wear_data[wear_data.SPEAKER == \"고객\"].SENTENCE\n",
        "store = wear_data[wear_data.SPEAKER == \"점원\"].SENTENCE\n",
        "print(customer.shape, store.shape) # 질문의 개수와 답의 개수가 일치하지 않는다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15826, 20)\n",
            "(8381,) (7445,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLNSiqcHM4WY",
        "outputId": "0583fd67-7851-4e61-a431-d86af1cab5c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prev = \"고객\"\n",
        "store_arr = []\n",
        "customer_arr = []\n",
        "store_stc = \"\"\n",
        "customer_stc = \"\"\n",
        "\n",
        "for i in range(wear_data.shape[0]):\n",
        "    if (prev == wear_data.iloc[i].SPEAKER):\n",
        "        if prev == \"점원\":\n",
        "             store_stc += (\" \"+wear_data.iloc[i].SENTENCE)\n",
        "        else : \n",
        "             customer_stc += (\" \"+wear_data.iloc[i].SENTENCE)\n",
        "            \n",
        "    elif prev == \"점원\":\n",
        "        store_arr.append(store_stc)\n",
        "        customer_stc = wear_data.iloc[i].SENTENCE\n",
        "        prev = \"고객\"\n",
        "    else :\n",
        "        customer_arr.append(customer_stc)\n",
        "        store_stc = wear_data.iloc[i].SENTENCE\n",
        "        prev = \"점원\"\n",
        "\n",
        "print(len(store_arr))\n",
        "print(len(customer_arr))\n",
        "print(store_arr[-1])\n",
        "print(customer_arr[-1]) # 자료 상에서 이후에는 계속 고객의 물음만 계속된다. 코드 레벨에서 이 부분은 빼게 구현했다. (stc는 만들어지지만 arr에 append 안하게 된다.)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7301\n",
            "7301\n",
            "요즘 파스텔 톤이 유행이에요\n",
            "요즘 유행하는 색깔이 뭐예요?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNvig2FMzjXj",
        "outputId": "e2d30656-fb75-4c1a-eb4b-265e58d8decf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "question = []\n",
        "answer = []\n",
        "\n",
        "for Q in customer_arr:\n",
        "    question.append(Q.replace(\"[^\\w]\", \" \"))\n",
        "\n",
        "for A in store_arr:\n",
        "    answer.append(A.replace(\"[^\\w]\", \" \"))\n",
        "\n",
        "len(question), len(answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7301, 7301)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbivSckMydzN"
      },
      "source": [
        "# 형태소분석 함수\n",
        "def pos_tag(sentences):\n",
        "    \n",
        "    # KoNLPy 형태소분석기 설정\n",
        "    tagger = Okt()\n",
        "    \n",
        "    # 문장 품사 변수 초기화\n",
        "    sentences_pos = []\n",
        "    \n",
        "    # 모든 문장 반복\n",
        "    for sentence in sentences:\n",
        "        # [\\\"':;~()] 특수기호 제거\n",
        "        sentence = re.sub(RE_FILTER, \"\", sentence)\n",
        "        \n",
        "        # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
        "        sentence = \" \".join(tagger.morphs(sentence))\n",
        "        sentences_pos.append(sentence)\n",
        "        \n",
        "    return sentences_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4seqSNWcydzP",
        "outputId": "f6b7b5f3-2c3d-4068-e4d2-6369b48dff4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 형태소분석 수행\n",
        "question = pos_tag(question)\n",
        "answer = pos_tag(answer)\n",
        "\n",
        "# 형태소분석으로 변환된 챗봇 데이터 출력\n",
        "for i in range(5):\n",
        "    print('Q : ' + question[i])\n",
        "    print('A : ' + answer[i])\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q : 신발 은 여기 있는 게 다예 요 ?\n",
            "A : 네 성인 이나 아동 다 있어요 발 사이즈 몇 신으세요 ?\n",
            "\n",
            "Q : 230 이요\n",
            "A : 편하게 신 을 수 있는 거 찾으세요 ?\n",
            "\n",
            "Q : 네 봄 이니까 편하게 신 을 수 있는 거\n",
            "A : 이런 건 어떠세요 ? 이런 거도 신발 무척 편하거든요\n",
            "\n",
            "Q : 굽 좀 높은 거 없나요 ?\n",
            "A : 봄 상품 은 아직 어른 제품 이 많이 안 나왔습니다\n",
            "\n",
            "Q : 언제 들어와요 ?\n",
            "A : 이번 주 지나면 들어올 거 예요\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrnct_nzydzR"
      },
      "source": [
        "# 질문과 대답 문장들을 하나로 합침\n",
        "sentences = []\n",
        "sentences.extend(question)\n",
        "sentences.extend(answer)\n",
        "\n",
        "words = []\n",
        "\n",
        "# 단어들의 배열 생성\n",
        "for sentence in sentences:\n",
        "    for word in sentence.split():\n",
        "        words.append(word)\n",
        "\n",
        "# 길이가 0인 단어는 삭제\n",
        "words = [word for word in words if len(word) > 0]\n",
        "\n",
        "# 중복된 단어 삭제\n",
        "words = list(set(words))\n",
        "\n",
        "# 제일 앞에 태그 단어 삽입\n",
        "words[:0] = [PAD, STA, END, OOV]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE9ZhJpEy_rT",
        "outputId": "585997a1-d326-4ded-86eb-43d5f6b89279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "VOCAB_SIZE = len(words)\n",
        "print(\"손님과 점원의 말에서 사용된 총 단어의 수 :\",len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "손님과 점원의 말에서 사용된 총 단어의 수 : 6409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkvyOnSvydzX"
      },
      "source": [
        "# 단어와 인덱스의 딕셔너리 생성\n",
        "word_to_index = {word: index for index, word in enumerate(words)}\n",
        "index_to_word = {index: word for index, word in enumerate(words)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfoTztrvydzc"
      },
      "source": [
        "# 문장을 인덱스로 변환\n",
        "def convert_text_to_index(sentences, vocabulary, type): \n",
        "    \n",
        "    sentences_index = []\n",
        "    \n",
        "    # 모든 문장에 대해서 반복\n",
        "    for sentence in sentences:\n",
        "        sentence_index = []\n",
        "        \n",
        "        # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
        "        if type == DECODER_INPUT:\n",
        "            sentence_index.extend([vocabulary[STA]])\n",
        "        \n",
        "        # 문장의 단어들을 띄어쓰기로 분리\n",
        "        for word in sentence.split():\n",
        "            if vocabulary.get(word) is not None:\n",
        "                # 사전에 있는 단어면 해당 인덱스를 추가\n",
        "                sentence_index.extend([vocabulary[word]])\n",
        "            else:\n",
        "                # 사전에 없는 단어면 OOV 인덱스를 추가\n",
        "                sentence_index.extend([vocabulary[OOV]])\n",
        "\n",
        "        # 최대 길이 검사\n",
        "        if type == DECODER_TARGET:\n",
        "            # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
        "            if len(sentence_index) >= max_sequences:\n",
        "                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]\n",
        "            else:\n",
        "                sentence_index += [vocabulary[END]]\n",
        "        else:\n",
        "            if len(sentence_index) > max_sequences:\n",
        "                sentence_index = sentence_index[:max_sequences]\n",
        "            \n",
        "        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
        "        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]\n",
        "        \n",
        "        # 문장의 인덱스 배열을 추가\n",
        "        sentences_index.append(sentence_index)\n",
        "\n",
        "    return np.asarray(sentences_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zezFPTvcydzf",
        "outputId": "cb11b371-eeb3-4b26-c042-7a4dec4fe6cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 인코더 입력 인덱스 변환\n",
        "x_encoder = convert_text_to_index(question, word_to_index, ENCODER_INPUT)\n",
        "\n",
        "# 첫 번째 인코더 입력 출력 (신발 은 여기 있는 게 다예 요)\n",
        "print(question[0])\n",
        "x_encoder[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "신발 은 여기 있는 게 다예 요 ?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4371, 2130, 2183, 2079, 1961, 3255, 2511, 4923,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmql1Lf_ydzh",
        "outputId": "df8f55db-769a-4c2e-f4c9-a71778b0f867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 디코더 입력 인덱스 변환\n",
        "x_decoder = convert_text_to_index(answer, word_to_index, DECODER_INPUT)\n",
        "\n",
        "# 첫 번째 디코더 입력 출력 (<START> 신발 은 여기 있는 게 다예 요)\n",
        "print(question[0])\n",
        "x_decoder[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "신발 은 여기 있는 게 다예 요 ?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1, 3824, 1065, 6038, 3033, 6040, 4593, 6096, 2148, 1116, 2222,\n",
              "       4923,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qIhDbaLydzk",
        "outputId": "9e2df16c-8490-4f0c-fcda-e491376e2698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 디코더 목표 인덱스 변환\n",
        "y_decoder = convert_text_to_index(answer, word_to_index, DECODER_TARGET)\n",
        "\n",
        "# 첫 번째 디코더 입력 출력 (신발 은 여기 있는 게 다예 요 <END>)\n",
        "print(question[0])\n",
        "y_decoder[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "신발 은 여기 있는 게 다예 요 ?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3824, 1065, 6038, 3033, 6040, 4593, 6096, 2148, 1116, 2222, 4923,\n",
              "          2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsf4tJl61Xi9"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsxmXWS41XVd"
      },
      "source": [
        "# decoder inputs use the previous target as input\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': x_encoder,\n",
        "        'dec_inputs': x_decoder\n",
        "    },\n",
        "    {\n",
        "        'outputs': y_decoder\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cstXcPJo4rKD",
        "outputId": "9dc7f7e4-1a09-458a-c894-b968193953f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ({inputs: (None, 30), dec_inputs: (None, 30)}, {outputs: (None, 30)}), types: ({inputs: tf.int64, dec_inputs: tf.int64}, {outputs: tf.int64})>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxXCQWvw2jwx"
      },
      "source": [
        "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH-k-kk_1_Vu"
      },
      "source": [
        "## scaled dot product Attention\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True) # QK^T\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth) #  QK^T / sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9) # zero padding token softmax 결과가 0이 나오도록\n",
        "  \n",
        "  attention_weights = tf.nn.softmax(logits, axis = -1) # softmax(QK^T / sqrt(d_k))\n",
        "\n",
        "  output = tf.matmul(attention_weights, value) # softmax(QK^T / sqrt(d_k)) * V\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7orcKMr13xY8"
      },
      "source": [
        "## multi-head attention\n",
        "## each head need (scaled_dot_product_attention)\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0 # 128,8\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "  \n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(inputs, shape=(batch_size,-1,self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3]) ##????\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    #linear\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    #split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    #scaled dot-product attention\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    #concatenation of heads\n",
        "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "    #final linear\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmlW0oi89nEC"
      },
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37sh3f8P-JUB",
        "outputId": "f9ad109b-7b5b-4063-ad7b-8fbe039a134c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 0. 1. 0. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W7ld50z3vT2"
      },
      "source": [
        "# it handle mask future tokens in a sequence used decoder. and mask pad tokens\n",
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W25teKwt_F80",
        "outputId": "1515fc2e-8591-4a6f-9ee9-93d13fd15114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 0. 1.]\n",
            "   [0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLhXEIJgASo3"
      },
      "source": [
        "Positional encoding\n",
        "\n",
        "since we don't use any rnn, cnn, positional encoding give model position information of words in sentence.\n",
        "\n",
        "positional encoding vector is added to embedding vector\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7s19-x3_Hpq"
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "  \n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles #pos/10000^(2i/d_model)\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position = tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model = d_model)\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLiumNZZDZGY"
      },
      "source": [
        "### Encoder Layer\n",
        "1. Multi-head attention (with padding mask)\n",
        "2. 2 dense layers followed by dropout\n",
        "\n",
        "also has residual connection followd by a layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oarRWUMLDYnC"
      },
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query':inputs,\n",
        "          'key':inputs,\n",
        "          'value':inputs,\n",
        "          'mask':padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3rO9IcDHGE5"
      },
      "source": [
        "### Encoder\n",
        "1. Input Embedding\n",
        "2. Positional Encoding\n",
        "3. `num_layers` encoder layers\n",
        "\n",
        "Embedding + positional encoding : input\n",
        "\n",
        "going encoder layers.\n",
        "\n",
        "output going decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOwoi2_jHvbA"
      },
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)#??왜 vocab_size가 들어가지?\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units = units,\n",
        "        d_model = d_model,\n",
        "        num_heads = num_heads,\n",
        "        dropout = dropout,\n",
        "        name = \"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "  \n",
        "  return tf.keras.Model(\n",
        "      inputs = [inputs, padding_mask], outputs = outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8XHH5dpJr-G"
      },
      "source": [
        "### Decoder Layer\n",
        "1. Masked multi-head attention (with look ahead mask and padding mask)\n",
        "2. Multi-head attention (with padding mask). `value` and `key` is from encoder output. `query` is from Multi-head attention layer output\n",
        "3. 2 dense layers followed by dropout\n",
        "\n",
        "also has residual connection followd by a layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtAfKdk-JrxK"
      },
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(shape=(1,None,None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1,1,None), name=\"padding_mask\")\n",
        "\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query' : inputs,\n",
        "          'key' : inputs,\n",
        "          'value' : inputs,\n",
        "          'mask' : look_ahead_mask\n",
        "      })\n",
        "  attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query' : attention1,\n",
        "          'key' : enc_outputs,\n",
        "          'value' : enc_outputs,\n",
        "          'mask' : padding_mask\n",
        "      })\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs = [inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs = outputs,\n",
        "      name = name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnBSBcm1NAYv"
      },
      "source": [
        "### Decoder\n",
        "1. output Embedding\n",
        "2. Positional Encoding\n",
        "3. `num_layers` decoder layers\n",
        "\n",
        "Embedding + positional encoding : input (target)\n",
        "\n",
        "going decoder layers.\n",
        "\n",
        "output going final linear layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnA-8FEAOT4F"
      },
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"decoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(shape=(1,None,None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1,1,None), name='padding_mask')\n",
        "\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units = units,\n",
        "        d_model = d_model,\n",
        "        num_heads = num_heads,\n",
        "        dropout = dropout,\n",
        "        name = \"decoder_layer_{}\".format(i),\n",
        "    )(inputs = [outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "  \n",
        "  return tf.keras.Model(\n",
        "      inputs = [inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs = outputs,\n",
        "      name = name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb-vmtqKQjhf"
      },
      "source": [
        "### Transformer\n",
        "1. encoder\n",
        "2. decoder\n",
        "3. final linear layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEMfL5SFQqr4"
      },
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1,1,None),\n",
        "      name=\"enc_padding_mask\")(inputs)\n",
        "  \n",
        "  #mask future tokens for decoder inputs at 1st attention block\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask, output_shape=(1,None,None),\n",
        "      name=\"look_ahead_mask\")(dec_inputs)\n",
        "  \n",
        "  #mask encoder outputs for the 2nd attention block\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1,1,None),\n",
        "      name=\"dec_padding_mask\")(inputs)\n",
        "  \n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pav-8Jd3ydzp"
      },
      "source": [
        "## 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8y-mjWtydzp"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK3IwtA4TOnN"
      },
      "source": [
        "### Loss function\n",
        "since target sequences are padded, deal this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi7YnqpVTYn2"
      },
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, max_sequences))\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "  \n",
        "  mask = tf.cast(tf.not_equal(y_true,0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzHEmCzUVhTu"
      },
      "source": [
        "### Custom learning rate\n",
        "use Adam optimizer with custom learning rate\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkFm9m_LVt8c"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb8PGbKgWAbP"
      },
      "source": [
        "### Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vql4FYFUV_3_"
      },
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
        "  y_true = tf.reshape(y_true, shape=(-1, max_sequences))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTXbb1Z2I4IM"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl3V7BllWWFG",
        "outputId": "25dd8d94-5770-4818-8e72-a5dc4ab65d5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 1.9702 - accuracy: 0.0223\n",
            "Epoch 2/50\n",
            "115/115 [==============================] - 8s 66ms/step - loss: 1.7318 - accuracy: 0.0334\n",
            "Epoch 3/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 1.4763 - accuracy: 0.0423\n",
            "Epoch 4/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 1.3232 - accuracy: 0.0459\n",
            "Epoch 5/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 1.2270 - accuracy: 0.0572\n",
            "Epoch 6/50\n",
            "115/115 [==============================] - 7s 65ms/step - loss: 1.1367 - accuracy: 0.0677\n",
            "Epoch 7/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 1.0552 - accuracy: 0.0753\n",
            "Epoch 8/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.9846 - accuracy: 0.0819\n",
            "Epoch 9/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.9222 - accuracy: 0.0871\n",
            "Epoch 10/50\n",
            "115/115 [==============================] - 7s 65ms/step - loss: 0.8641 - accuracy: 0.0922\n",
            "Epoch 11/50\n",
            "115/115 [==============================] - 8s 65ms/step - loss: 0.8096 - accuracy: 0.0969\n",
            "Epoch 12/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.7570 - accuracy: 0.1013\n",
            "Epoch 13/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.7062 - accuracy: 0.1066\n",
            "Epoch 14/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.6575 - accuracy: 0.1116\n",
            "Epoch 15/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.6063 - accuracy: 0.1178\n",
            "Epoch 16/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.5592 - accuracy: 0.1240\n",
            "Epoch 17/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.5088 - accuracy: 0.1314\n",
            "Epoch 18/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.4600 - accuracy: 0.1390\n",
            "Epoch 19/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.4102 - accuracy: 0.1467\n",
            "Epoch 20/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.3652 - accuracy: 0.1549\n",
            "Epoch 21/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.3185 - accuracy: 0.1642\n",
            "Epoch 22/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.2786 - accuracy: 0.1720\n",
            "Epoch 23/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.2412 - accuracy: 0.1793\n",
            "Epoch 24/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.2073 - accuracy: 0.1861\n",
            "Epoch 25/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.1836 - accuracy: 0.1911\n",
            "Epoch 26/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.1656 - accuracy: 0.1940\n",
            "Epoch 27/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.1473 - accuracy: 0.1981\n",
            "Epoch 28/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.1332 - accuracy: 0.2008\n",
            "Epoch 29/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.1269 - accuracy: 0.2018\n",
            "Epoch 30/50\n",
            "115/115 [==============================] - 8s 66ms/step - loss: 0.1176 - accuracy: 0.2042\n",
            "Epoch 31/50\n",
            "115/115 [==============================] - 8s 67ms/step - loss: 0.1142 - accuracy: 0.2042\n",
            "Epoch 32/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.1070 - accuracy: 0.2060\n",
            "Epoch 33/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.1046 - accuracy: 0.2067\n",
            "Epoch 34/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0986 - accuracy: 0.2079\n",
            "Epoch 35/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0999 - accuracy: 0.2073\n",
            "Epoch 36/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0955 - accuracy: 0.2084\n",
            "Epoch 37/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0902 - accuracy: 0.2100\n",
            "Epoch 38/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0807 - accuracy: 0.2124\n",
            "Epoch 39/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0791 - accuracy: 0.2131\n",
            "Epoch 40/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0761 - accuracy: 0.2138\n",
            "Epoch 41/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0686 - accuracy: 0.2161\n",
            "Epoch 42/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0657 - accuracy: 0.2169\n",
            "Epoch 43/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0649 - accuracy: 0.2171\n",
            "Epoch 44/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0606 - accuracy: 0.2182\n",
            "Epoch 45/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0564 - accuracy: 0.2193\n",
            "Epoch 46/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0569 - accuracy: 0.2193\n",
            "Epoch 47/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0539 - accuracy: 0.2200\n",
            "Epoch 48/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0489 - accuracy: 0.2213\n",
            "Epoch 49/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0495 - accuracy: 0.2215\n",
            "Epoch 50/50\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.0461 - accuracy: 0.2225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdfa6d1eeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx-jxXncYmlp"
      },
      "source": [
        "## Category Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seok_9PxYrlV"
      },
      "source": [
        "main = wear_data['MAIN']\n",
        "category = wear_data['CATEGORY']\n",
        "all_stc = wear_data['SENTENCE']\n",
        "\n",
        "category_info = pd.DataFrame({\"stc\":all_stc, \"cate\":main})\n",
        "rough_info = pd.DataFrame({\"stc\":all_stc, \"cate\":category})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uawcREfY87l",
        "outputId": "4e40886b-10bc-45db-8570-270ffe324a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "category_info.dropna(inplace = True)\n",
        "category_info.reset_index(drop=True, inplace = True)\n",
        "\n",
        "rough_info.dropna(inplace = True)\n",
        "rough_info.reset_index(drop=True, inplace = True)\n",
        "\n",
        "print(category_info.shape, rough_info.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15725, 2) (15826, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-h0d_06Y_GA"
      },
      "source": [
        "def make_tokenize(info):\n",
        "    tagger = Okt()\n",
        "\n",
        "    for i in range(info.shape[0]):\n",
        "        info['stc'][i] = tagger.morphs(info['stc'][i])\n",
        "\n",
        "make_tokenize(category_info)\n",
        "make_tokenize(rough_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmGrD5juZ_ET"
      },
      "source": [
        "category_list = pd.factorize(category_info['cate'])[1]\n",
        "category_info['cate'] = pd.factorize(category_info['cate'])[0]\n",
        "\n",
        "rough_category_list = pd.factorize(rough_info['cate'])[1]\n",
        "rough_info['cate'] = pd.factorize(rough_info['cate'])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVVyo4EftIvZ",
        "outputId": "20fcfdb3-5b03-4cbf-a3be-ef49c0f7aed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rough_info['cate']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3        0\n",
              "4        0\n",
              "        ..\n",
              "15821    2\n",
              "15822    2\n",
              "15823    2\n",
              "15824    2\n",
              "15825    2\n",
              "Name: cate, Length: 15826, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQLKF7FkbRAi"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "category_ans = to_categorical(category_info['cate']) # 카테고리 관련 원핫벡터 카테고리\n",
        "rough_ans = to_categorical(rough_info['cate']) # 4개 카테고리 관련 원핫벡터 카테고리"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZavgRPJdM79"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "ctokenizer = Tokenizer(5000) \n",
        "ctokenizer.fit_on_texts(category_info['stc'])\n",
        "ctoken_stc = ctokenizer.texts_to_sequences(category_info['stc'])\n",
        "category_stc = ctoken_stc\n",
        "\n",
        "rtokenizer = Tokenizer(5000) \n",
        "rtokenizer.fit_on_texts(rough_info['stc'])\n",
        "rtoken_stc = rtokenizer.texts_to_sequences(rough_info['stc'])\n",
        "rough_stc = rtoken_stc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soNw3c6Cen_G"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = 15\n",
        "category_stc = pad_sequences(category_stc, maxlen=max_len) # 카테고리 관련 패딩까지 마친 문장 모음\n",
        "rough_stc = pad_sequences(rough_stc, maxlen=max_len) # 4개 카테고리 관련 패딩까지 마친 문장 모음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2SP7QT_a-4n"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cX_train, cX_test, cy_train, cy_test = train_test_split(\n",
        "    category_stc, category_ans, test_size = 0.2, shuffle = True, random_state = 11)\n",
        "\n",
        "rX_train, rX_test, ry_train, ry_test = train_test_split(\n",
        "    rough_stc, rough_ans, test_size = 0.2, shuffle = True, random_state = 11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW-r-YFCfVgv"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "\n",
        "cmodel = Sequential()\n",
        "cmodel.add(Embedding(5000, 128))\n",
        "cmodel.add(LSTM(128))\n",
        "cmodel.add(Dense(405, activation='softmax'))\n",
        "\n",
        "rmodel = Sequential()\n",
        "rmodel.add(Embedding(5000, 100))\n",
        "rmodel.add(LSTM(128))\n",
        "rmodel.add(Dense(4, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHQoXs8wfkYk"
      },
      "source": [
        "cmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "cmodel.fit(cX_train, cy_train, validation_data=(cX_test, cy_test), batch_size=32, epochs=30) # 128 64 32 실험"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYqy-6NEfrXJ"
      },
      "source": [
        "rmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "rmodel.fit(rX_train, ry_train, validation_data=(rX_test, ry_test), batch_size=10, epochs=10) # 128 64 32 실험"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXEMQH_jf5jl"
      },
      "source": [
        "def find_category(stc,tokenizer,model,category_list):\n",
        "    tagger = Okt()\n",
        "    stc = tagger.morphs(stc)\n",
        "    encode_stc = tokenizer.texts_to_sequences([stc])\n",
        "    pad_stc = pad_sequences(encode_stc, maxlen=15)\n",
        "    score = model.predict(pad_stc)\n",
        "    return (category_list[score.argmax()], score[0, score.argmax()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIlguUGEf-ow"
      },
      "source": [
        "sentence = \"이 코트에 어울리는 치마 있나요?\"\n",
        "print(find_category(sentence,ctokenizer,cmodel,category_list))\n",
        "sentence = \"같은 디자인으로 혹시 더 저렴한 상품 있나요?\"\n",
        "print(find_category(sentence,ctokenizer,cmodel,category_list))\n",
        "sentence = \"겨울이니까 좀 길게 입으셔야죠\"\n",
        "print(find_category(sentence,ctokenizer,cmodel,category_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODrJYTQ6f_70"
      },
      "source": [
        "sentence = \"이 코트에 어울리는 치마 있나요?\"\n",
        "print(find_category(sentence,rtokenizer,rmodel,rough_category_list))\n",
        "sentence = \"굽 좀 높은 거 없나요?\"\n",
        "print(find_category(sentence,rtokenizer,rmodel,rough_category_list))\n",
        "sentence = \"겨울이니까 좀 길게 입으셔야죠\"\n",
        "print(find_category(sentence,rtokenizer,rmodel,rough_category_list))\n",
        "sentence = \"순금으로 된 제품인가요?\"\n",
        "print(find_category(sentence,rtokenizer,rmodel,rough_category_list))\n",
        "sentence = \"이 제품 얼마인가요?\"\n",
        "print(find_category(sentence,rtokenizer,rmodel,rough_category_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9ggzKDnuFOJ"
      },
      "source": [
        "rX_train, rX_test, ry_train, ry_test = train_test_split(\n",
        "    rough_stc, rough_info['cate'], test_size = 0.2, shuffle = True, random_state = 11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC6So8latxl0"
      },
      "source": [
        "tmpX = np.array(rX_train)\n",
        "tmpX_test = np.array(rX_test)\n",
        "tmpy = np.array(ry_train)\n",
        "tmpy_test = np.array(ry_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfnxQU3CqwAG",
        "outputId": "9975222c-2502-4602-fc9f-57fca76e4878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "# 1000 0.01-> 0.4\n",
        "# 100 0.01 -> 0.4\n",
        "# 1 0.01 -> 0.4\n",
        "# 0.1 0.01 -> 0.38 테케도 못맞춤\n",
        "# 1 1 -> 0.39\n",
        "# 100 1 -> 0.38\n",
        "# 1000 10 -> 0.39\n",
        "# 1000 1 -> 0.39\n",
        "maxC = 0\n",
        "maxG = 0\n",
        "maxJ = 0\n",
        "jungdo = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "for i in jungdo:\n",
        "    for j in jungdo:\n",
        "        model_svm = SVC(C=i, gamma=j).fit(tmpX, tmpy)\n",
        "        if(maxJ < model_svm.score(tmpX_test, tmpy_test)):\n",
        "            maxJ = model_svm.score(tmpX_test, tmpy_test)\n",
        "            maxC = i\n",
        "            maxG = j\n",
        "        print(i,j)\n",
        "\n",
        "print(maxC, maxG, maxJ)\n",
        "# model_svm = SVC(C=1000, gamma=1000).fit(tmpX, tmpy)\n",
        "\n",
        "        # 평가\n",
        "# print(\"훈련 세트 정확도: {:.2f}\".format(model_svm.score(tmpX, tmpy)))\n",
        "# print(\"테스트 세트 정확도: {:.2f}\".format(model_svm.score(tmpX_test, tmpy_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.001 0.001\n",
            "0.001 0.01\n",
            "0.001 0.1\n",
            "0.001 1\n",
            "0.001 10\n",
            "0.001 100\n",
            "0.001 100\n",
            "0.001 1000\n",
            "0.01 0.001\n",
            "0.01 0.01\n",
            "0.01 0.1\n",
            "0.01 1\n",
            "0.01 10\n",
            "0.01 100\n",
            "0.01 100\n",
            "0.01 1000\n",
            "0.1 0.001\n",
            "0.1 0.01\n",
            "0.1 0.1\n",
            "0.1 1\n",
            "0.1 10\n",
            "0.1 100\n",
            "0.1 100\n",
            "0.1 1000\n",
            "1 0.001\n",
            "1 0.01\n",
            "1 0.1\n",
            "1 1\n",
            "1 10\n",
            "1 100\n",
            "1 100\n",
            "1 1000\n",
            "10 0.001\n",
            "10 0.01\n",
            "10 0.1\n",
            "10 1\n",
            "10 10\n",
            "10 100\n",
            "10 100\n",
            "10 1000\n",
            "100 0.001\n",
            "100 0.01\n",
            "100 0.1\n",
            "100 1\n",
            "100 10\n",
            "100 100\n",
            "100 100\n",
            "100 1000\n",
            "100 0.001\n",
            "100 0.01\n",
            "100 0.1\n",
            "100 1\n",
            "100 10\n",
            "100 100\n",
            "100 100\n",
            "100 1000\n",
            "1000 0.001\n",
            "1000 0.01\n",
            "1000 0.1\n",
            "1000 1\n",
            "1000 10\n",
            "1000 100\n",
            "1000 100\n",
            "1000 1000\n",
            "1 0.001 0.40271636133922933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQfyv8Ui56kX",
        "outputId": "1ca0c2a9-10a6-483e-8c9c-bf8da598c2e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_svm = SVC(C=1, gamma=0.001).fit(tmpX, tmpy)\n",
        "\n",
        "        # 평가\n",
        "print(\"훈련 세트 정확도: {:.2f}\".format(model_svm.score(tmpX, tmpy)))\n",
        "print(\"테스트 세트 정확도: {:.2f}\".format(model_svm.score(tmpX_test, tmpy_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 세트 정확도: 0.90\n",
            "테스트 세트 정확도: 0.40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AsHAp3R7ge_",
        "outputId": "42d02079-e975-46e1-e00a-112a6229391f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmpy_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 3, ..., 0, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlOAX3CD7U8X",
        "outputId": "2d5d6d0c-7af9-41c4-dd39-c972ebf20389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 모델 학습\n",
        "# model = AdaBoostClassifier(n_estimators=5, random_state=42)\n",
        "# model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model = DecisionTreeClassifier(max_depth = 5)\n",
        "model.fit(tmpX, tmpy)\n",
        "\n",
        "# 평가\n",
        "# print(\"훈련 세트 정확도: {:.3f}\".format(model.score(tmpX, tmpy)))\n",
        "print(\"테스트 세트 정확도: {:.3f}\".format(model.score(tmpX_test, tmpX_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-af684932e706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# print(\"훈련 세트 정확도: {:.3f}\".format(model.score(tmpX, tmpy)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"테스트 세트 정확도: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and multiclass targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEehl-Sv6aY-",
        "outputId": "f97a7080-0cf6-419c-b027-0cfa3093abf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "model.fit(tmpX, tmpy)\n",
        "\n",
        "# 평가\n",
        "print(\"훈련 세트 정확도: {:.3f}\".format(model.score(tmpX, tmpy)))\n",
        "print(\"테스트 세트 정확도: {:.3f}\".format(model.score(tmpX_test, tmpX_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 세트 정확도: 0.511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-59538417e485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"훈련 세트 정확도: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"테스트 세트 정확도: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and multiclass targets"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6EDnEOwI_G7"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxi2B-vjydzt"
      },
      "source": [
        "# 인덱스를 문장으로 변환\n",
        "def convert_index_to_text(indexs, vocabulary): \n",
        "    \n",
        "    sentence = ''\n",
        "    \n",
        "    # 모든 문장에 대해서 반복\n",
        "    for index in indexs:\n",
        "        if index == END_INDEX:\n",
        "            # 종료 인덱스면 중지\n",
        "            break;\n",
        "        if vocabulary.get(index) is not None:\n",
        "            # 사전에 있는 인덱스면 해당 단어를 추가\n",
        "            sentence += vocabulary[index]\n",
        "        else:\n",
        "            # 사전에 없는 인덱스면 OOV 단어를 추가\n",
        "            sentence.extend([vocabulary[OOV_INDEX]])\n",
        "            \n",
        "        # 빈칸 추가\n",
        "        sentence += ' '\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E66RpbXydzy"
      },
      "source": [
        "# 예측을 위한 입력 생성\n",
        "def make_predict_input(sentence):\n",
        "\n",
        "    sentences = []\n",
        "    sentences.append(sentence)\n",
        "    sentences = pos_tag(sentences)\n",
        "    input_seq = convert_text_to_index(sentences, word_to_index, ENCODER_INPUT)\n",
        "    \n",
        "    return input_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umBNECObKCJj"
      },
      "source": [
        "def evaluate(input_seq):\n",
        "\n",
        "  input_seq = input_seq.squeeze()\n",
        "  sentence = tf.expand_dims(input_seq, axis=0)\n",
        "  output = tf.expand_dims([1], 0)\n",
        "\n",
        "  for i in range(max_sequences):\n",
        "    predictions = model(inputs=[sentence, output], training=False)\n",
        "\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if tf.equal(predicted_id, 2):\n",
        "      break\n",
        "\n",
        "    # concatenated the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBxzil_NWNLX",
        "outputId": "f1ad3d7f-ef6c-4c0f-df16-6c3ee4023385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp_test_stc = convert_index_to_text(evaluate(make_predict_input('내일 뭐해요?')).numpy()[1:],index_to_word)\n",
        "print(tmp_test_stc)\n",
        "print(find_category(tmp_test_stc,ctokenizer,cmodel,category_list))\n",
        "print(find_category(tmp_test_stc,rtokenizer,rmodel,rough_category_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "내일 은 문 닫아요 \n",
            "('공휴일영업문의', 0.9520013)\n",
            "('액세서리', 0.7485255)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUDsxhUxWaa9",
        "outputId": "7210214e-f390-4ee9-9183-84f8ff030740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp_test_stc = convert_index_to_text(evaluate(make_predict_input('이거 사이즈 큰거 있어요?')).numpy()[1:],index_to_word)\n",
        "print(tmp_test_stc)\n",
        "print(find_category(tmp_test_stc,ctokenizer,cmodel,category_list))\n",
        "print(find_category(tmp_test_stc,rtokenizer,rmodel,rough_category_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "아니요 여기 서 또 할인 되세요 \n",
            "('구매제품할인문의', 0.36791855)\n",
            "('의류', 0.56272423)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFym9Kw4Wkb9",
        "outputId": "0e9b9d07-3a9f-4ddb-89c3-c025f4d536ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp_test_stc = convert_index_to_text(evaluate(make_predict_input('이거 사이즈 230 있어요?')).numpy()[1:],index_to_word)\n",
        "print(tmp_test_stc)\n",
        "print(find_category(tmp_test_stc,ctokenizer,cmodel,category_list))\n",
        "print(find_category(tmp_test_stc,rtokenizer,rmodel,rough_category_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "아니요 프리 사이즈 예요 \n",
            "('치수문의', 0.99659)\n",
            "('의류', 0.9994836)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPixonr7g-FT",
        "outputId": "93f62e42-5f2e-4452-ae8a-6fca64f14e7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp_test_stc = convert_index_to_text(evaluate(make_predict_input('할인 되요?')).numpy()[1:],index_to_word)\n",
        "print(tmp_test_stc)\n",
        "print(find_category(tmp_test_stc,ctokenizer,cmodel,category_list))\n",
        "print(find_category(tmp_test_stc,rtokenizer,rmodel,rough_category_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전 제품 세 일 기간 이라 할인 가능합니다 \n",
            "('할인여부문의', 0.8501277)\n",
            "('의류', 0.38548493)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3YqOZ-MEOVW",
        "outputId": "37525c2f-4233-442e-f747-07640d1b275a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# for train data predict\n",
        "for seq_index in range(0,10):\n",
        "\n",
        "  print(\"고객 : \",question[seq_index])\n",
        "  print(find_category(question[seq_index],ctokenizer,cmodel,category_list))\n",
        "  print(find_category(question[seq_index],rtokenizer,rmodel,rough_category_list))\n",
        "  print(\"정답점원 :\",answer[seq_index])\n",
        "  print(find_category(answer[seq_index],ctokenizer,cmodel,category_list))\n",
        "  print(find_category(answer[seq_index],rtokenizer,rmodel,rough_category_list))\n",
        "  print(\"AI점원 :\",convert_index_to_text(evaluate(make_predict_input(question[seq_index])).numpy()[1:],index_to_word))\n",
        "  print(find_category(convert_index_to_text(evaluate(make_predict_input(question[seq_index])).numpy()[1:],index_to_word),ctokenizer,cmodel,category_list))\n",
        "  print(find_category(convert_index_to_text(evaluate(make_predict_input(question[seq_index])).numpy()[1:],index_to_word),rtokenizer,rmodel,rough_category_list))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "고객 :  신발 은 여기 있는 게 다예 요 ?\n",
            "('종류별신발제품문의요청', 0.9467577)\n",
            "('신발', 0.99834406)\n",
            "정답점원 : 네 성인 이나 아동 다 있어요 발 사이즈 몇 신으세요 ?\n",
            "('종류별신발제품문의요청', 0.9994634)\n",
            "('신발', 0.99467564)\n",
            "AI점원 : 네 , 그거 는 검정색 하나 에요 \n",
            "('동제품타색상문의', 0.51743716)\n",
            "('신발', 0.8278762)\n",
            "\n",
            "\n",
            "고객 :  230 이요\n",
            "('종류별신발제품문의요청', 0.9270687)\n",
            "('신발', 0.9999192)\n",
            "정답점원 : 편하게 신 을 수 있는 거 찾으세요 ?\n",
            "('착화감', 0.94413644)\n",
            "('신발', 0.9954848)\n",
            "AI점원 : 이 신발 신어 보시겠어요 ? \n",
            "('종류별신발제품문의요청', 0.85774106)\n",
            "('신발', 0.99987257)\n",
            "\n",
            "\n",
            "고객 :  네 봄 이니까 편하게 신 을 수 있는 거\n",
            "('착화감', 0.9897278)\n",
            "('신발', 0.99729437)\n",
            "정답점원 : 이런 건 어떠세요 ? 이런 거도 신발 무척 편하거든요\n",
            "('의상과코디할제품문의', 0.55929387)\n",
            "('신발', 0.9998684)\n",
            "AI점원 : 이런 건 어떠세요 ? 이런 거도 신발 무척 편하거든요 \n",
            "('의상과코디할제품문의', 0.55929387)\n",
            "('신발', 0.9998684)\n",
            "\n",
            "\n",
            "고객 :  굽 좀 높은 거 없나요 ?\n",
            "('굽높이문의', 0.98385495)\n",
            "('신발', 0.9999933)\n",
            "정답점원 : 봄 상품 은 아직 어른 제품 이 많이 안 나왔습니다\n",
            "('굽높이문의', 0.9914833)\n",
            "('신발', 0.9941881)\n",
            "AI점원 : 봄 상품 은 아직 어른 제품 이 많이 안 나왔습니다 \n",
            "('굽높이문의', 0.9914833)\n",
            "('신발', 0.9941881)\n",
            "\n",
            "\n",
            "고객 :  언제 들어와요 ?\n",
            "('재입고문의', 0.98333675)\n",
            "('신발', 0.37694752)\n",
            "정답점원 : 이번 주 지나면 들어올 거 예요\n",
            "('재입고문의', 0.98705786)\n",
            "('신발', 0.9583851)\n",
            "AI점원 : 이번 주 지나면 들어올 거 예요 \n",
            "('재입고문의', 0.98705786)\n",
            "('신발', 0.9583851)\n",
            "\n",
            "\n",
            "고객 :  이 거 는 가죽 이에요 ?\n",
            "('소재문의', 0.81088746)\n",
            "('신발', 0.5071697)\n",
            "정답점원 : 가죽 아니고 쎄 무예 요\n",
            "('소재문의', 0.99893373)\n",
            "('신발', 0.99959344)\n",
            "AI점원 : 그거 는 합피 예요 인조 가죽 그래서 물 에 좀 강하고요 \n",
            "('가방소재문의', 0.985721)\n",
            "('가방', 0.9999614)\n",
            "\n",
            "\n",
            "고객 :  가죽 은 얼마 예요 ?\n",
            "('제품가격문의', 0.90057415)\n",
            "('신발', 0.7311646)\n",
            "정답점원 : 2만 9천 원 입니다\n",
            "('제품가격문의', 0.9999789)\n",
            "('신발', 0.6722458)\n",
            "AI점원 : 2만 9천 원 입니다 \n",
            "('제품가격문의', 0.9999789)\n",
            "('신발', 0.6722458)\n",
            "\n",
            "\n",
            "고객 :  털 달린 거 저 거 는 사이즈 있어요 ?\n",
            "('사이즈재고문의', 0.99071443)\n",
            "('신발', 0.89916974)\n",
            "정답점원 : 230 이 없어요 이 거 한 번 신어 보세요\n",
            "('사이즈문의', 0.5780728)\n",
            "('신발', 0.99999404)\n",
            "AI점원 : 230 이 없어요 이 거 한 번 신어 보세요 \n",
            "('사이즈문의', 0.5780728)\n",
            "('신발', 0.99999404)\n",
            "\n",
            "\n",
            "고객 :  좀 크네 또 안 들어와요 ?\n",
            "('재입고문의', 0.7328751)\n",
            "('신발', 0.9982529)\n",
            "정답점원 : 네 이건 다 끝났어요\n",
            "('재입고문의', 0.9002159)\n",
            "('신발', 0.98427296)\n",
            "AI점원 : 네 이건 다 끝났어요 \n",
            "('재입고문의', 0.9002159)\n",
            "('신발', 0.98427296)\n",
            "\n",
            "\n",
            "고객 :  가방 매는 거 보고 있어요\n",
            "('종류별가방제품문의요청', 0.9996006)\n",
            "('가방', 0.9998691)\n",
            "정답점원 : 여기 있어요\n",
            "('용도별가방요청문의', 0.15775186)\n",
            "('의류', 0.48678753)\n",
            "AI점원 : 여기 있습니다 \n",
            "('제품별색깔종류문의', 0.17246757)\n",
            "('의류', 0.43007722)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcHilTFm6GKP",
        "outputId": "31b020b9-c2a6-454c-a077-1404a885e376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp_of_all = '안녕하세요'\n",
        "# tmp_test_stc = convert_index_to_text(evaluate(make_predict_input(tmp_of_all)).numpy()[1:],index_to_word)\n",
        "print(tmp_of_all)\n",
        "print(find_category(tmp_of_all,ctokenizer,cmodel,category_list))\n",
        "print(find_category(tmp_of_all,rtokenizer,rmodel,rough_category_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "안녕하세요\n",
            "('결제요청', 0.17259958)\n",
            "('신발', 0.98255193)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYevuQqbFW88"
      },
      "source": [
        "tmp_test_stc = convert_index_to_text(evaluate(make_predict_input('할인 되요?')).numpy()[1:],index_to_word)\n",
        "print(tmp_test_stc)\n",
        "print(find_category(tmp_test_stc,ctokenizer,cmodel,category_list))\n",
        "print(find_category(tmp_test_stc,rtokenizer,rmodel,rough_category_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QESaSt38Jme",
        "outputId": "edc0ed73-ca29-43fd-d304-059b94341d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "question_list1 = ['근무하신지 얼마나 되셨어요?','이가게 월세가 얼마에요?','내일 저랑 데이트 하실래요?','여기 화장실이 어디에요?','이 가방 최대 몇 리터까지 들어갈까요?','이거 끈은 분리하고 세탁기에 돌려야 하나요?','이거 방수 되는 거죠?','우리 애가 이번에 초등학교 들어가는데 가방 추천좀 해주세요.','내구성 좋은 가방은 요즘 어떤게 잘나가요?']\n",
        "question_list2 = ['이 라이더 자켓 어제 샀는데 영수증 없어도 환불 되나요?','가볍고 따뜻한 패딩조끼 있나요?,물 잘 안빠지는 청바지 있나요?','이 스웨터 여기 살짝 올이 나갔는데 할인 안해주시나요?','가게 앞 마네킹에 피팅된 바지 다른 색깔 있나요?,수선 맡기고 옷 택배로 받아 볼 수 있나요?','지역상품권 결제 가능한가요?','카운터에 있는 점원이 입고 있는 옷 맘에 드는데 어떤 제품인가요?','소개팅에 입고 나갈만한 옷 추천 가능한가요?','이 가게 단골이고 사장님이랑 잘 아는데 현금 결제하면 할인 없나요?']\n",
        "question_list3= ['침은 모두 알러지 방지 침인가요?','로즈골드 색상도 있나요?','귀걸이는 어떻게 관리해야 하나요?','요즘 잘나가는 제품은 뭔가요?,이 목걸이랑 세트인 귀걸이도 있나요?']\n",
        "question_list4= ['신어보고 구매 가능할까요?','겨울에 신기는 너무 춥죠?','밖에 날씨가 너무 춥네요','신발이 너무 가벼운거같아요 좀 무게감있는건 없나요?','다른브랜드에서는 비슷한게있나요?']\n",
        "input_list_all = ['근무하신지 얼마나 되셨어요?','이가게 월세가 얼마에요?','내일 저랑 데이트 하실래요?','여기 화장실이 어디에요?',\n",
        "                  '이 가방 최대 몇 리터까지 들어갈까요?','이거 끈은 분리하고 세탁기에 돌려야 하나요?',\n",
        "                  '이거 방수 되는 거죠?','우리 애가 이번에 초등학교 들어가는데 가방 추천좀 해주세요.',\n",
        "                  '내구성 좋은 가방은 요즘 어떤게 잘나가요?','이 라이더 자켓 어제 샀는데 영수증 없어도 환불 되나요?',\n",
        "                  '가볍고 따뜻한 패딩조끼 있나요?','물 잘 안빠지는 청바지 있나요?','이 스웨터 여기 살짝 올이 나갔는데 할인 안해주시나요?',\n",
        "                  '가게 앞 마네킹에 피팅된 바지 다른 색깔 있나요?','수선 맡기고 옷 택배로 받아 볼 수 있나요?','지역상품권 결제 가능한가요?',\n",
        "                  '카운터에 있는 점원이 입고 있는 옷 맘에 드는데 어떤 제품인가요?','소개팅에 입고 나갈만한 옷 추천 가능한가요?',\n",
        "                  '이 가게 단골이고 사장님이랑 잘 아는데 현금 결제하면 할인 없나요?','침은 모두 알러지 방지 침인가요??',\n",
        "                  '로즈골드 색상도 있나요?','귀걸이는 어떻게 관리해야 하나요?','요즘 잘나가는 제품은 뭔가요?',\n",
        "                  '이 목걸이랑 세트인 귀걸이도 있나요?']\n",
        "question_list = [question_list1,question_list2,question_list3,question_list4]\n",
        "\n",
        "percent = 0\n",
        "\n",
        "for j in question_list:\n",
        "    for i in j:\n",
        "        tmp_test_stc = convert_index_to_text(evaluate(make_predict_input(i)).numpy()[1:],index_to_word)\n",
        "        print(i)\n",
        "        print(find_category(i,ctokenizer,cmodel,category_list))\n",
        "        print(find_category(i,rtokenizer,rmodel,rough_category_list))\n",
        "        print(tmp_test_stc)\n",
        "        print(find_category(tmp_test_stc,ctokenizer,cmodel,category_list))\n",
        "        print(find_category(tmp_test_stc,rtokenizer,rmodel,rough_category_list))\n",
        "        if(find_category(i,rtokenizer,rmodel,rough_category_list)[0] == find_category(tmp_test_stc,rtokenizer,rmodel,rough_category_list)[0] ):\n",
        "            percent+=1\n",
        "        print()\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "print(percent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "근무하신지 얼마나 되셨어요?\n",
            "('영업기간문의', 0.8513088)\n",
            "('신발', 0.99740547)\n",
            "여기 온 지는 삼 년 됐어요 \n",
            "('운영기간문의', 0.99351877)\n",
            "('가방', 0.9960478)\n",
            "\n",
            "이가게 월세가 얼마에요?\n",
            "('신발과관련된MD구매', 0.5846967)\n",
            "('가방', 0.49484193)\n",
            "오십 프로 해서 육만 구천 원 이에요 \n",
            "('제품가격문의', 0.99708897)\n",
            "('가방', 0.8043634)\n",
            "\n",
            "내일 저랑 데이트 하실래요?\n",
            "('시즌별카탈로그요구', 0.207884)\n",
            "('가방', 0.38008243)\n",
            "네 \n",
            "('제품가격문의', 0.06582175)\n",
            "('의류', 0.423783)\n",
            "\n",
            "여기 화장실이 어디에요?\n",
            "('화장실위치문의', 0.99481344)\n",
            "('의류', 0.69682735)\n",
            "여기 밖에 있어요 \n",
            "('화장실위치문의', 0.88811153)\n",
            "('의류', 0.43634188)\n",
            "\n",
            "이 가방 최대 몇 리터까지 들어갈까요?\n",
            "('제품사용방법문의', 0.47253865)\n",
            "('가방', 0.9998165)\n",
            "이 가방 은 14만 8천 원 이에요 \n",
            "('제품가격문의', 0.99776065)\n",
            "('가방', 0.99896693)\n",
            "\n",
            "이거 끈은 분리하고 세탁기에 돌려야 하나요?\n",
            "('가방끈문의', 0.39510274)\n",
            "('신발', 0.61759657)\n",
            "네 , 우리 옷 는 약간 디자인 옷 들 이에요 \n",
            "('수제화문의', 0.23560458)\n",
            "('의류', 0.9999964)\n",
            "\n",
            "이거 방수 되는 거죠?\n",
            "('방수제품문의', 0.9986099)\n",
            "('가방', 0.9992785)\n",
            "네 \n",
            "('제품가격문의', 0.06582175)\n",
            "('의류', 0.423783)\n",
            "\n",
            "우리 애가 이번에 초등학교 들어가는데 가방 추천좀 해주세요.\n",
            "('선물할대상에따른제품문의', 0.38436836)\n",
            "('가방', 0.9993686)\n",
            "사이즈 가 어떻게 되시나요 ? \n",
            "('매장에서착용문의', 0.24881357)\n",
            "('신발', 0.67877084)\n",
            "\n",
            "내구성 좋은 가방은 요즘 어떤게 잘나가요?\n",
            "('유사제품추천요청', 0.8676933)\n",
            "('가방', 0.9985164)\n",
            "네 일요일 색상 을 찾으세요 ? \n",
            "('종류별가방제품문의요청', 0.47900152)\n",
            "('의류', 0.99910754)\n",
            "\n",
            "\n",
            "\n",
            "이 라이더 자켓 어제 샀는데 영수증 없어도 환불 되나요?\n",
            "('예약제품배송문의', 0.32768214)\n",
            "('의류', 0.99984956)\n",
            "네 \n",
            "('제품가격문의', 0.06582175)\n",
            "('의류', 0.423783)\n",
            "\n",
            "가볍고 따뜻한 패딩조끼 있나요?,물 잘 안빠지는 청바지 있나요?\n",
            "('인기제품문의', 0.4695389)\n",
            "('의류', 0.9999969)\n",
            "기모 같은 옷 으로 입으시면 좀 크게 나왔어요 \n",
            "('스타일링문의', 0.72188276)\n",
            "('의류', 0.99997735)\n",
            "\n",
            "이 스웨터 여기 살짝 올이 나갔는데 할인 안해주시나요?\n",
            "('제품구매시할인문의', 0.6232716)\n",
            "('의류', 0.99997544)\n",
            "정가 은 이미 품절 입니다 \n",
            "('구매제품할인문의', 0.3018653)\n",
            "('신발', 0.51116455)\n",
            "\n",
            "가게 앞 마네킹에 피팅된 바지 다른 색깔 있나요?,수선 맡기고 옷 택배로 받아 볼 수 있나요?\n",
            "('각인문의', 0.48363063)\n",
            "('의류', 0.9999162)\n",
            "원래 옷 이나 정장 을 보여 드릴 까요 ? \n",
            "('사이즈적합여부문의', 0.35140038)\n",
            "('의류', 0.999974)\n",
            "\n",
            "지역상품권 결제 가능한가요?\n",
            "('상품권사용문의', 0.98358744)\n",
            "('의류', 0.4347542)\n",
            "네 , 그럼요 . \n",
            "('화장시피팅문의', 0.75654453)\n",
            "('신발', 0.39306158)\n",
            "\n",
            "카운터에 있는 점원이 입고 있는 옷 맘에 드는데 어떤 제품인가요?\n",
            "('코디제품추천문의', 0.2962077)\n",
            "('신발', 0.50550026)\n",
            "캐 주얼 한 바지 는 오른쪽 에 준비 되어 있습니다 \n",
            "('종류별의류제품문의요청', 0.99536926)\n",
            "('의류', 0.94149053)\n",
            "\n",
            "소개팅에 입고 나갈만한 옷 추천 가능한가요?\n",
            "('코디제품추천문의', 0.3312806)\n",
            "('의류', 0.99999106)\n",
            "네 . 구 스 다운 덕 다운 그리고 그냥 패딩 이 있어요 \n",
            "('계절상품문의', 0.84838253)\n",
            "('의류', 0.99986315)\n",
            "\n",
            "이 가게 단골이고 사장님이랑 잘 아는데 현금 결제하면 할인 없나요?\n",
            "('수선가능문의', 0.32109636)\n",
            "('의류', 0.47677088)\n",
            "네 고객 님 지금 옷 입어보시면 돼요 \n",
            "('피팅문의', 0.6398809)\n",
            "('의류', 0.99999964)\n",
            "\n",
            "\n",
            "\n",
            "침은 모두 알러지 방지 침인가요?\n",
            "('알레르기유무문의', 0.84966946)\n",
            "('액세서리', 0.9999881)\n",
            "네 잠시 만 기다려 주세요 \n",
            "('매장에서착용문의', 0.3799344)\n",
            "('신발', 0.5913631)\n",
            "\n",
            "로즈골드 색상도 있나요?\n",
            "('색상에대한문의', 0.9854424)\n",
            "('액세서리', 0.9979322)\n",
            "네 있습니다 잠시 만 기다려주세요 \n",
            "('매장에서착용문의', 0.19447815)\n",
            "('신발', 0.33903563)\n",
            "\n",
            "귀걸이는 어떻게 관리해야 하나요?\n",
            "('제품보관방법문의', 0.76345795)\n",
            "('액세서리', 0.9994436)\n",
            "니트 류 는 드라이 가 나아요 \n",
            "('제품의세탁방법문의', 0.9783651)\n",
            "('의류', 0.9999957)\n",
            "\n",
            "요즘 잘나가는 제품은 뭔가요?,이 목걸이랑 세트인 귀걸이도 있나요?\n",
            "('종류별액세서리제품문의요청', 0.3440617)\n",
            "('액세서리', 0.99979144)\n",
            "네 . 세트 도 가능합니다 . \n",
            "('세트제품문의', 0.94730794)\n",
            "('의류', 0.4203573)\n",
            "\n",
            "\n",
            "\n",
            "신어보고 구매 가능할까요?\n",
            "('매장에서착용문의', 0.95968145)\n",
            "('신발', 0.9998273)\n",
            "55 랑 66 있어요 \n",
            "('치수문의', 0.8692704)\n",
            "('의류', 0.99999833)\n",
            "\n",
            "겨울에 신기는 너무 춥죠?\n",
            "('계절상품문의', 0.9943399)\n",
            "('의류', 0.9606072)\n",
            "그럼 이 쪽 디자인 은 어떠신 가요 ? \n",
            "('종류별액세서리제품문의요청', 0.3575696)\n",
            "('액세서리', 0.7933686)\n",
            "\n",
            "밖에 날씨가 너무 춥네요\n",
            "('종류별의류제품문의요청', 0.98435426)\n",
            "('의류', 0.93838245)\n",
            "네 고객 님 사이즈 가 어떻게 되시나요 ? \n",
            "('매장에서착용문의', 0.5348037)\n",
            "('신발', 0.695094)\n",
            "\n",
            "신발이 너무 가벼운거같아요 좀 무게감있는건 없나요?\n",
            "('재입고문의', 0.62001765)\n",
            "('신발', 0.9999387)\n",
            "네 , 무난 한 색상 이라 가격 대가 어떻게 되세요 ? \n",
            "('제품가격문의', 0.9991543)\n",
            "('신발', 0.5480777)\n",
            "\n",
            "다른브랜드에서는 비슷한게있나요?\n",
            "('다른브랜드제품문의', 0.97593915)\n",
            "('신발', 0.9109132)\n",
            "네 고객 님 이 옷 에 다른 색상 도 있습니다 \n",
            "('제품별색깔종류문의', 0.7286073)\n",
            "('의류', 0.999915)\n",
            "\n",
            "\n",
            "\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYv7EewQhIAd"
      },
      "source": [
        "## Test data prediction\n",
        " - 문장중 14900행부터의 문장 100개에 대한 예측."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uZiQH0Ziu_h"
      },
      "source": [
        "test_question_arr = []\n",
        "test_question = \"\"\n",
        "\n",
        "for i in range(14900,wear_data.shape[0]):\n",
        "        test_question = wear_data.iloc[i].SENTENCE\n",
        "        test_question_arr.append(test_question)\n",
        "\n",
        "# test_question_arr = pos_tag(test_question_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RroUo2wthdA-",
        "outputId": "bc275185-51fe-4424-81c7-1c79aa85b228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# for train data predict\n",
        "for seq_index in range(0,10):\n",
        "\n",
        "  print(\"고객 : \",test_question_arr[seq_index])\n",
        "  print(find_category(test_question_arr[seq_index],ctokenizer,cmodel,category_list))\n",
        "  print(find_category(test_question_arr[seq_index],rtokenizer,rmodel,rough_category_list))\n",
        "  print(\"AI점원 :\",convert_index_to_text(evaluate(make_predict_input(test_question_arr[seq_index])).numpy()[1:],index_to_word))\n",
        "  print(find_category(convert_index_to_text(evaluate(make_predict_input(test_question_arr[seq_index])).numpy()[1:],index_to_word),ctokenizer,cmodel,category_list))\n",
        "  print(find_category(convert_index_to_text(evaluate(make_predict_input(test_question_arr[seq_index])).numpy()[1:],index_to_word),rtokenizer,rmodel,rough_category_list))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "고객 :  구제는 뭐예요?\n",
            "('신제품,구제품관련문의', 0.99233705)\n",
            "('의류', 0.99975735)\n",
            "AI점원 : 예 \n",
            "('원하는제품재고여부확인', 0.120788075)\n",
            "('가방', 0.471872)\n",
            "\n",
            "\n",
            "고객 :  구제 옷도 있네요?\n",
            "('신제품,구제품관련문의', 0.99220467)\n",
            "('의류', 0.99997735)\n",
            "AI점원 : 예 , 그 옷 은 봄 옷 이 전부 에요 \n",
            "('계절상품문의', 0.3881231)\n",
            "('의류', 0.99920744)\n",
            "\n",
            "\n",
            "고객 :  구제품은 다 세탁해서 나온 거예요?\n",
            "('구제품세탁여부문의', 0.9989729)\n",
            "('의류', 0.99877924)\n",
            "AI점원 : 네 고객 님 께 저 제품 소재 가 요즘 에는 아이보리 가 낫고요 \n",
            "('제품특징문의', 0.6134685)\n",
            "('신발', 0.96028894)\n",
            "\n",
            "\n",
            "고객 :  이건 다 세탁이 된 거죠?\n",
            "('구제품세탁여부문의', 0.973141)\n",
            "('의류', 0.9701384)\n",
            "AI점원 : 네 \n",
            "('가방소재문의', 0.054704018)\n",
            "('의류', 0.42276567)\n",
            "\n",
            "\n",
            "고객 :  구제품은 세탁해서 파시는 건가요?\n",
            "('구제품세탁여부문의', 0.99079835)\n",
            "('의류', 0.9975472)\n",
            "AI점원 : 네 고객 님 팔고 있습니다 \n",
            "('운동종목별기능성제품문의', 0.8318357)\n",
            "('액세서리', 0.9776799)\n",
            "\n",
            "\n",
            "고객 :  구제품은 다 세탁해서 나오는 거겠죠?\n",
            "('구제품세탁여부문의', 0.9833261)\n",
            "('의류', 0.99890816)\n",
            "AI점원 : 네 고객 님 이 제품 이 잘 나가요 \n",
            "('베스트상품문의', 0.6179648)\n",
            "('가방', 0.45370793)\n",
            "\n",
            "\n",
            "고객 :  카드로 결제하면 부가세 별도에요?\n",
            "('카드결제부가세여부문의', 0.99447745)\n",
            "('의류', 0.93950886)\n",
            "AI점원 : 카드 결제 해도 똑같은 가격 이에요 \n",
            "('현금할인가문의', 0.93692863)\n",
            "('의류', 0.9871682)\n",
            "\n",
            "\n",
            "고객 :  카드는 부가세 별도로 붙나요?\n",
            "('카드결제부가세여부문의', 0.98946524)\n",
            "('의류', 0.9998839)\n",
            "AI점원 : 카드 로 결제 도 되고 여기 도 하셔도 돼요 \n",
            "('결제방식문의', 0.41915923)\n",
            "('의류', 0.98943514)\n",
            "\n",
            "\n",
            "고객 :  카드 결제하면 부가세 추가되나요?\n",
            "('카드결제부가세여부문의', 0.9935308)\n",
            "('의류', 0.99344337)\n",
            "AI점원 : 카드 결제 해도 똑같은 가격 이에요 \n",
            "('현금할인가문의', 0.93692863)\n",
            "('의류', 0.9871682)\n",
            "\n",
            "\n",
            "고객 :  카드로 하면 부가세 붙어요?\n",
            "('카드결제부가세여부문의', 0.995992)\n",
            "('의류', 0.999887)\n",
            "AI점원 : 네 , 카드 로 결제 도 와 드리겠습니다 \n",
            "('결제요청', 0.41338375)\n",
            "('가방', 0.4895228)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAs_p0bLhlPy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}